```{r}
# Load required libraries
library(ranger)    # for Random Forest
library(e1071)     # for SVM
library(xgboost)   # for XGBoost
library(caret)
library(dplyr)

# Read and preprocess data
data <- read.csv("C:/Users/ASUS/Desktop/BIO615/Group Project/abalone.csv", header = TRUE)

# Set target variable as numeric for regression
data$Rings <- as.numeric(data$Rings)

# Separate features and target variable
X <- data %>% select(-Rings)
y <- data$Rings

# Convert categorical variable (Sex) to numeric
X <- X %>%
  mutate(Sex = as.numeric(factor(Sex, levels = c("M", "F", "I"))))

# Split data into training and testing sets
set.seed(42)
train_index <- createDataPartition(y, p = 0.8, list = FALSE)
X_train <- X[train_index, ]
X_test <- X[-train_index, ]
y_train <- y[train_index]
y_test <- y[-train_index]

# 1. Use Ranger (Fast Random Forest) for Regression
rf_model <- ranger(
  dependent.variable.name = "Rings",
  data = cbind(X_train, Rings = y_train),
  num.trees = 500,
  mtry = floor(sqrt(ncol(X_train))),
  importance = 'impurity'  # Use impurity as a measure of importance
)

# Prediction and evaluation
rf_pred <- predict(rf_model, data = X_test)$predictions
rf_rmse <- RMSE(rf_pred, y_test)
print("Ranger (Fast Random Forest) RMSE:")
print(rf_rmse)

# Feature importance
importance_scores <- data.frame(
  Feature = names(rf_model$variable.importance),
  Importance = rf_model$variable.importance
)
importance_scores <- importance_scores[order(importance_scores$Importance, decreasing = TRUE), ]
print("Feature Importance:")
print(importance_scores)

# 2. Use Support Vector Machine (SVM) for Regression
# Scale the data
scale_values <- preProcess(X_train)
X_train_scaled <- predict(scale_values, X_train)
X_test_scaled <- predict(scale_values, X_test)

svm_model <- svm(x = X_train_scaled,
                 y = y_train,
                 kernel = "radial",
                 cost = 1,
                 type = "eps-regression",
                 scale = FALSE)

svm_pred <- predict(svm_model, X_test_scaled)
svm_rmse <- RMSE(svm_pred, y_test)
print("SVM RMSE:")
print(svm_rmse)

# 3. Use XGBoost for Regression
# Convert data to DMatrix format
dtrain <- xgb.DMatrix(data = as.matrix(X_train), label = y_train)
dtest <- xgb.DMatrix(data = as.matrix(X_test), label = y_test)

# Set XGBoost parameters
xgb_params <- list(
  objective = "reg:squarederror",  # For regression
  eta = 0.3,
  max_depth = 6,
  min_child_weight = 1,
  subsample = 0.8,
  colsample_bytree = 0.8
)

# Train the XGBoost model
xgb_model <- xgb.train(
  params = xgb_params,
  data = dtrain,
  nrounds = 100,
  watchlist = list(train = dtrain, test = dtest),
  early_stopping_rounds = 10,
  verbose = 0
)

# Make predictions
xgb_pred <- predict(xgb_model, as.matrix(X_test))
xgb_rmse <- RMSE(xgb_pred, y_test)
print("XGBoost RMSE:")
print(xgb_rmse)

# Feature importance for XGBoost
importance_matrix <- xgb.importance(model = xgb_model)
print("XGBoost Feature Importance:")
print(importance_matrix)

# Compare model performances
results <- data.frame(
  Model = c("Ranger (Fast RF)", "SVM", "XGBoost"),
  RMSE = c(rf_rmse, svm_rmse, xgb_rmse)
)
print("Model Comparison:")
print(results)
```

```{r}
# Calculate additional metrics
rf_mae <- MAE(rf_pred, y_test)
rf_r2 <- R2(rf_pred, y_test)

svm_mae <- MAE(svm_pred, y_test)
svm_r2 <- R2(svm_pred, y_test)

xgb_mae <- MAE(xgb_pred, y_test)
xgb_r2 <- R2(xgb_pred, y_test)

# Print additional metrics
print("Random Forest MAE and R2:")
print(rf_mae)
print(rf_r2)

print("SVM MAE and R2:")
print(svm_mae)
print(svm_r2)

print("XGBoost MAE and R2:")
print(xgb_mae)
print(xgb_r2)

# Update the results dataframe to include MAE and R2
results <- data.frame(
  Model = c("Ranger (Fast RF)", "SVM", "XGBoost"),
  RMSE = c(rf_rmse, svm_rmse, xgb_rmse),
  MAE = c(rf_mae, svm_mae, xgb_mae),
  R2 = c(rf_r2, svm_r2, xgb_r2)
)
print("Model Performance Comparison:")
print(results)
```