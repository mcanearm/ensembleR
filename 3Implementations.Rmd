```{r}
# Load required libraries
library(ranger)  # replace randomForest
library(e1071)
library(xgboost)
library(caret)
library(dplyr)

# Read and preprocess the data
data <- read.csv("C:/Users/ASUS/Desktop/BIO615/Group Project/heart_disease.csv")

# Convert target variable to factor
data$num <- as.factor(data$num)

# Handle missing values
data <- na.omit(data)

# Split data into features and target
X <- data %>% select(-num)
y <- data$num

# Create training and test sets
set.seed(42)
train_index <- createDataPartition(y, p = 0.8, list = FALSE)
X_train <- X[train_index, ]
X_test <- X[-train_index, ]
y_train <- y[train_index]
y_test <- y[-train_index]

# 1. Ranger (Fast Random Forest)
rf_model <- ranger(
  dependent.variable.name = "num",
  data = cbind(X_train, num = y_train),
  num.trees = 500,
  mtry = floor(sqrt(ncol(X_train))),
  importance = 'impurity',  # Use impurity as a measure of importance
  probability = TRUE  # If probabilistic predictions are needed
)

# prediction and evaluation
rf_pred <- predict(rf_model, data = X_test)
rf_predictions <- factor(max.col(rf_pred$predictions), levels = levels(y_test))
rf_cm <- confusionMatrix(rf_predictions, y_test)
print("Ranger (Fast Random Forest) Results:")
print(rf_cm)

# Feature importance
importance_scores <- data.frame(
  Feature = names(rf_model$variable.importance),
  Importance = rf_model$variable.importance
)
importance_scores <- importance_scores[order(importance_scores$Importance, decreasing = TRUE), ]
print("Feature Importance:")
print(importance_scores)

# 2. Support Vector Machine
# Scale the data
scale_values <- preProcess(X_train)
X_train_scaled <- predict(scale_values, X_train)
X_test_scaled <- predict(scale_values, X_test)

svm_model <- svm(x = X_train_scaled,
                 y = y_train,
                 kernel = "radial",
                 cost = 1,
                 scale = FALSE)

svm_pred <- predict(svm_model, X_test_scaled)
svm_cm <- confusionMatrix(svm_pred, y_test)
print("SVM Results:")
print(svm_cm)

# 3. XGBoost
# Convert data to DMatrix format
dtrain <- xgb.DMatrix(data = as.matrix(X_train), 
                      label = as.numeric(y_train) - 1)
dtest <- xgb.DMatrix(data = as.matrix(X_test), 
                     label = as.numeric(y_test) - 1)

# Set XGBoost parameters
xgb_params <- list(
  objective = "multi:softmax",
  num_class = length(levels(y)),
  eta = 0.3,
  max_depth = 6,
  min_child_weight = 1,
  subsample = 0.8,
  colsample_bytree = 0.8
)

# Train XGBoost model
xgb_model <- xgb.train(
  params = xgb_params,
  data = dtrain,
  nrounds = 100,
  watchlist = list(train = dtrain, test = dtest),
  early_stopping_rounds = 10,
  verbose = 0
)

# Make predictions
xgb_pred <- predict(xgb_model, as.matrix(X_test))
xgb_pred <- factor(xgb_pred, levels = 0:(length(levels(y))-1))
xgb_cm <- confusionMatrix(xgb_pred, y_test)
print("XGBoost Results:")
print(xgb_cm)

# Feature importance for XGBoost
importance_matrix <- xgb.importance(model = xgb_model)
print("XGBoost Feature Importance:")
print(importance_matrix)

# Compare model performances
results <- data.frame(
  Model = c("Ranger (Fast RF)", "SVM", "XGBoost"),
  Accuracy = c(rf_cm$overall["Accuracy"],
               svm_cm$overall["Accuracy"],
               xgb_cm$overall["Accuracy"]),
  Kappa = c(rf_cm$overall["Kappa"],
            svm_cm$overall["Kappa"],
            xgb_cm$overall["Kappa"])
)
print("Model Comparison:")
print(results)
```